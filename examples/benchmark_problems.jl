#!/usr/bin/env julia

"""
Pararealãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å•é¡Œé›†

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€Pararealã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ€§èƒ½è©•ä¾¡ç”¨ã®
æ¨™æº–çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å•é¡Œã‚’æä¾›ã—ã¾ã™ã€‚ç•°ãªã‚‹å•é¡Œã‚µã‚¤ã‚ºã€
ç‰©ç†ç‰¹æ€§ã€å¢ƒç•Œæ¡ä»¶ã§ã®æ€§èƒ½ã‚’ç³»çµ±çš„ã«è©•ä¾¡ã§ãã¾ã™ã€‚

å®Ÿè¡Œæ–¹æ³•:
    mpirun -np 4 julia benchmark_problems.jl [problem_name]
    
åˆ©ç”¨å¯èƒ½ãªå•é¡Œ:
    - small: å°è¦æ¨¡å•é¡Œï¼ˆ32Â³æ ¼å­ï¼‰
    - medium: ä¸­è¦æ¨¡å•é¡Œï¼ˆ64Â³æ ¼å­ï¼‰
    - large: å¤§è¦æ¨¡å•é¡Œï¼ˆ128Â³æ ¼å­ï¼‰
    - ic_package: ICãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç†±è§£æ
    - all: å…¨ã¦ã®å•é¡Œã‚’å®Ÿè¡Œ
"""

using MPI
using Heat3ds
using JSON
using Dates

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å•é¡Œã®å®šç¾©
struct BenchmarkProblem
    name::String
    description::String
    NX::Int
    NY::Int
    NZ::Int
    total_time::Float64
    thermal_diffusivity::Float64
    expected_speedup_range::Tuple{Float64, Float64}
    recommended_config::Dict{String, Any}
end

function get_benchmark_problems()
    """æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å•é¡Œã®å®šç¾©"""
    
    problems = BenchmarkProblem[]
    
    # å°è¦æ¨¡å•é¡Œ
    push!(problems, BenchmarkProblem(
        "small",
        "Small-scale problem for algorithm validation",
        32, 32, 32,
        0.5,  # 0.5ç§’
        1.0e-4,  # æ¨™æº–çš„ãªç†±æ‹¡æ•£ç‡
        (1.2, 2.5),  # æœŸå¾…ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ç¯„å›²
        Dict(
            "time_step_ratio" => 10.0,
            "n_time_windows" => 2,
            "max_iterations" => 15,
            "convergence_tolerance" => 1.0e-6
        )
    ))
    
    # ä¸­è¦æ¨¡å•é¡Œ
    push!(problems, BenchmarkProblem(
        "medium",
        "Medium-scale problem for performance evaluation",
        64, 64, 32,
        1.0,  # 1ç§’
        1.0e-4,
        (2.0, 4.0),
        Dict(
            "time_step_ratio" => 25.0,
            "n_time_windows" => 4,
            "max_iterations" => 20,
            "convergence_tolerance" => 1.0e-6
        )
    ))
    
    # å¤§è¦æ¨¡å•é¡Œ
    push!(problems, BenchmarkProblem(
        "large",
        "Large-scale problem for scalability testing",
        128, 64, 32,
        2.0,  # 2ç§’
        1.0e-4,
        (3.0, 8.0),
        Dict(
            "time_step_ratio" => 50.0,
            "n_time_windows" => 8,
            "max_iterations" => 25,
            "convergence_tolerance" => 1.0e-6
        )
    ))
    
    # ICãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å•é¡Œ
    push!(problems, BenchmarkProblem(
        "ic_package",
        "IC package thermal analysis benchmark",
        100, 100, 20,
        5.0,  # 5ç§’ï¼ˆé•·æ™‚é–“è§£æï¼‰
        1.4e-4,  # ã‚·ãƒªã‚³ãƒ³ã®ç†±æ‹¡æ•£ç‡
        (4.0, 12.0),
        Dict(
            "time_step_ratio" => 75.0,
            "n_time_windows" => 8,
            "max_iterations" => 30,
            "convergence_tolerance" => 1.0e-7
        )
    ))
    
    # é«˜ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”å•é¡Œ
    push!(problems, BenchmarkProblem(
        "high_aspect",
        "High aspect ratio problem (thin film)",
        128, 128, 8,
        1.0,
        5.0e-5,  # è–„è†œææ–™
        (2.5, 6.0),
        Dict(
            "time_step_ratio" => 40.0,
            "n_time_windows" => 6,
            "max_iterations" => 25,
            "convergence_tolerance" => 1.0e-6
        )
    ))
    
    return problems
end

function create_benchmark_config(problem::BenchmarkProblem, n_processes::Int)
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å•é¡Œç”¨ã®Pararealè¨­å®šã‚’ä½œæˆ"""
    
    # åŸºæœ¬æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã®è¨ˆç®—
    characteristic_length = 1.0 / max(problem.NX, problem.NY, problem.NZ)
    dt_fine = 0.1 * characteristic_length^2 / problem.thermal_diffusivity
    dt_coarse = dt_fine * problem.recommended_config["time_step_ratio"]
    
    # æ™‚é–“çª“æ•°ã®èª¿æ•´ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æ•°ã«å¿œã˜ã¦ï¼‰
    n_windows = min(problem.recommended_config["n_time_windows"], n_processes)
    
    config = PararealConfig(
        total_time=problem.total_time,
        n_time_windows=n_windows,
        dt_coarse=dt_coarse,
        dt_fine=dt_fine,
        time_step_ratio=problem.recommended_config["time_step_ratio"],
        max_iterations=problem.recommended_config["max_iterations"],
        convergence_tolerance=problem.recommended_config["convergence_tolerance"],
        n_mpi_processes=n_windows,
        n_threads_per_process=n_processes Ã· n_windows,
        auto_optimize_parameters=false,
        parameter_exploration_mode=false,
        validation_mode=true,
        enable_performance_profiling=true
    )
    
    return config
end

struct BenchmarkResult
    problem_name::String
    n_processes::Int
    n_threads_per_process::Int
    execution_time::Float64
    parareal_iterations::Int
    converged::Bool
    speedup::Float64
    efficiency::Float64
    l2_error::Float64
    max_error::Float64
    memory_usage::Float64
    within_expected_range::Bool
end

function run_benchmark(problem::BenchmarkProblem, comm, rank, size)
    """å˜ä¸€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å•é¡Œã®å®Ÿè¡Œ"""
    
    if rank == 0
        println("\n" * "="^60)
        println("Running benchmark: $(problem.name)")
        println("Description: $(problem.description)")
        println("Problem size: $(problem.NX)Ã—$(problem.NY)Ã—$(problem.NZ) = $(problem.NX*problem.NY*problem.NZ) points")
        println("Analysis time: $(problem.total_time) seconds")
        println("Expected speedup: $(problem.expected_speedup_range[1])x - $(problem.expected_speedup_range[2])x")
        println("="^60)
    end
    
    # Pararealè¨­å®š
    config = create_benchmark_config(problem, size)
    
    if rank == 0
        println("Configuration:")
        println("  Time windows: $(config.n_time_windows)")
        println("  Time step ratio: $(config.time_step_ratio)")
        println("  Fine time step: $(config.dt_fine)")
        println("  Coarse time step: $(config.dt_coarse)")
        println("  Max iterations: $(config.max_iterations)")
    end
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
    initial_memory = Sys.total_memory() - Sys.free_memory()
    
    # å®Ÿè¡Œæ™‚é–“æ¸¬å®š
    start_time = time()
    
    try
        # Pararealå®Ÿè¡Œ
        result = q3d(problem.NX, problem.NY, problem.NZ,
                    solver="pbicgstab",
                    epsilon=1.0e-8,
                    par="thread",
                    is_steady=false,
                    parareal=true,
                    parareal_config=config)
        
        execution_time = time() - start_time
        final_memory = Sys.total_memory() - Sys.free_memory()
        memory_usage = (final_memory - initial_memory) / 1024^3  # GB
        
        if rank == 0
            # çµæœã®è§£æ
            speedup = haskey(result, :performance_metrics) ? 
                     result.performance_metrics.overall_speedup : 0.0
            efficiency = speedup / config.n_time_windows
            
            l2_error = haskey(result, :validation_metrics) ? 
                      result.validation_metrics.l2_norm_error : 0.0
            max_error = haskey(result, :validation_metrics) ? 
                       result.validation_metrics.max_pointwise_error : 0.0
            
            # æœŸå¾…ç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯
            within_range = (problem.expected_speedup_range[1] <= speedup <= problem.expected_speedup_range[2])
            
            benchmark_result = BenchmarkResult(
                problem.name,
                size,
                Threads.nthreads(),
                execution_time,
                result.parareal_iterations,
                result.converged,
                speedup,
                efficiency,
                l2_error,
                max_error,
                memory_usage,
                within_range
            )
            
            # çµæœã®è¡¨ç¤º
            print_benchmark_result(benchmark_result, problem)
            
            return benchmark_result
        end
        
    catch e
        if rank == 0
            println("âŒ Benchmark failed: $e")
            
            # å¤±æ•—æ™‚ã®ãƒ€ãƒŸãƒ¼çµæœ
            return BenchmarkResult(
                problem.name, size, Threads.nthreads(),
                0.0, 0, false, 0.0, 0.0, Inf, Inf, 0.0, false
            )
        end
    end
    
    return nothing
end

function print_benchmark_result(result::BenchmarkResult, problem::BenchmarkProblem)
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®è¡¨ç¤º"""
    
    status = result.converged ? "âœ…" : "âŒ"
    range_status = result.within_expected_range ? "âœ…" : "âš ï¸"
    
    println("\n--- Results ---")
    println("$status Converged: $(result.converged)")
    println("   Iterations: $(result.iterations)")
    println("   Execution time: $(round(result.execution_time, digits=2)) seconds")
    println("   Speedup: $(round(result.speedup, digits=2))x $range_status")
    println("   Efficiency: $(round(result.efficiency*100, digits=1))%")
    println("   Memory usage: $(round(result.memory_usage, digits=2)) GB")
    
    if result.l2_error < Inf
        println("   L2 error: $(result.l2_error)")
        println("   Max error: $(result.max_error)")
    end
    
    # æ€§èƒ½è©•ä¾¡
    if result.converged
        if result.within_expected_range
            println("ğŸ‰ Performance: EXCELLENT (within expected range)")
        elseif result.speedup > problem.expected_speedup_range[1] * 0.8
            println("ğŸ‘ Performance: GOOD (close to expected range)")
        else
            println("âš ï¸  Performance: POOR (below expected range)")
        end
    else
        println("âŒ Performance: FAILED (did not converge)")
    end
end

function run_all_benchmarks(problems, comm, rank, size)
    """å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å•é¡Œã®å®Ÿè¡Œ"""
    
    results = BenchmarkResult[]
    
    if rank == 0
        println("Starting comprehensive benchmark suite...")
        println("Total problems: $(length(problems))")
        println("MPI processes: $size")
        println("Threads per process: $(Threads.nthreads())")
        println("Total cores: $(size * Threads.nthreads())")
    end
    
    for (i, problem) in enumerate(problems)
        if rank == 0
            println("\nProgress: $i/$(length(problems))")
        end
        
        result = run_benchmark(problem, comm, rank, size)
        if rank == 0 && result !== nothing
            push!(results, result)
        end
        
        # ãƒ—ãƒ­ã‚»ã‚¹é–“åŒæœŸã¨ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        MPI.Barrier(comm)
        GC.gc()
        
        # å•é¡Œé–“ã®ä¼‘æ†©
        sleep(1)
    end
    
    return results
end

function generate_benchmark_report(results)
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
    
    if isempty(results)
        println("No results to report")
        return
    end
    
    println("\n" * "="^80)
    println("BENCHMARK SUMMARY REPORT")
    println("="^80)
    
    # å…¨ä½“çµ±è¨ˆ
    total_tests = length(results)
    converged_tests = count(r -> r.converged, results)
    within_range_tests = count(r -> r.within_expected_range, results)
    
    println("Total tests: $total_tests")
    println("Converged: $converged_tests ($((converged_tests/total_tests*100) |> x -> round(x, digits=1))%)")
    println("Within expected range: $within_range_tests ($((within_range_tests/total_tests*100) |> x -> round(x, digits=1))%)")
    
    # åæŸã—ãŸçµæœã®ã¿ã§çµ±è¨ˆ
    converged_results = filter(r -> r.converged, results)
    
    if !isempty(converged_results)
        speedups = [r.speedup for r in converged_results]
        efficiencies = [r.efficiency for r in converged_results]
        
        println("\n--- Performance Statistics (Converged Tests Only) ---")
        println("Average speedup: $(round(sum(speedups)/length(speedups), digits=2))x")
        println("Best speedup: $(round(maximum(speedups), digits=2))x")
        println("Worst speedup: $(round(minimum(speedups), digits=2))x")
        println("Average efficiency: $(round(sum(efficiencies)/length(efficiencies)*100, digits=1))%")
        println("Best efficiency: $(round(maximum(efficiencies)*100, digits=1))%")
    end
    
    # å•é¡Œåˆ¥è©³ç´°çµæœ
    println("\n--- Detailed Results ---")
    println("Problem".ljust(15) * "Size".ljust(12) * "Speedup".ljust(10) * "Efficiency".ljust(12) * "Status")
    println("-"^60)
    
    for result in results
        size_str = "$(result.n_processes)Ã—$(result.n_threads_per_process)"
        speedup_str = result.converged ? "$(round(result.speedup, digits=2))x" : "N/A"
        efficiency_str = result.converged ? "$(round(result.efficiency*100, digits=1))%" : "N/A"
        status_str = result.converged ? (result.within_expected_range ? "âœ… PASS" : "âš ï¸ SLOW") : "âŒ FAIL"
        
        println(result.problem_name.ljust(15) * 
               size_str.ljust(12) * 
               speedup_str.ljust(10) * 
               efficiency_str.ljust(12) * 
               status_str)
    end
    
    # æ¨å¥¨äº‹é …
    println("\n--- Recommendations ---")
    
    best_problems = filter(r -> r.converged && r.within_expected_range, results)
    if !isempty(best_problems)
        best_overall = maximum(best_problems, by=r -> r.speedup)
        println("Best performing problem: $(best_overall.problem_name)")
        println("  Speedup: $(round(best_overall.speedup, digits=2))x")
        println("  Efficiency: $(round(best_overall.efficiency*100, digits=1))%")
    end
    
    poor_problems = filter(r -> r.converged && !r.within_expected_range, results)
    if !isempty(poor_problems)
        println("\nProblems needing optimization:")
        for p in poor_problems
            println("  $(p.problem_name): Consider adjusting time step ratio or window count")
        end
    end
    
    failed_problems = filter(r -> !r.converged, results)
    if !isempty(failed_problems)
        println("\nFailed problems:")
        for p in failed_problems
            println("  $(p.problem_name): Check convergence parameters or problem setup")
        end
    end
end

function save_benchmark_results(results, filename)
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®ä¿å­˜"""
    
    # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
    results_dict = []
    for r in results
        push!(results_dict, Dict(
            "problem_name" => r.problem_name,
            "n_processes" => r.n_processes,
            "n_threads_per_process" => r.n_threads_per_process,
            "execution_time" => r.execution_time,
            "parareal_iterations" => r.parareal_iterations,
            "converged" => r.converged,
            "speedup" => r.speedup,
            "efficiency" => r.efficiency,
            "l2_error" => r.l2_error,
            "max_error" => r.max_error,
            "memory_usage" => r.memory_usage,
            "within_expected_range" => r.within_expected_range
        ))
    end
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    output_data = Dict(
        "timestamp" => string(now()),
        "julia_version" => string(VERSION),
        "total_tests" => length(results),
        "converged_tests" => count(r -> r.converged, results),
        "system_info" => Dict(
            "total_memory" => Sys.total_memory(),
            "cpu_threads" => Sys.CPU_THREADS
        ),
        "results" => results_dict
    )
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    open(filename, "w") do f
        JSON.print(f, output_data, 2)
    end
    
    println("Benchmark results saved to: $filename")
end

function main()
    # MPIåˆæœŸåŒ–
    MPI.Init()
    comm = MPI.COMM_WORLD
    rank = MPI.Comm_rank(comm)
    size = MPI.Comm_size(comm)
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®å‡¦ç†
    problem_name = length(ARGS) > 0 ? ARGS[1] : "all"
    
    if rank == 0
        println("=== Parareal Benchmark Suite ===")
        println("Target problem: $problem_name")
        println("MPI processes: $size")
        println("Julia threads per process: $(Threads.nthreads())")
        println("Start time: $(now())")
    end
    
    try
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å•é¡Œã®å–å¾—
        all_problems = get_benchmark_problems()
        
        # å®Ÿè¡Œã™ã‚‹å•é¡Œã®é¸æŠ
        if problem_name == "all"
            problems = all_problems
        else
            problems = filter(p -> p.name == problem_name, all_problems)
            if isempty(problems)
                if rank == 0
                    println("Error: Unknown problem '$problem_name'")
                    println("Available problems: $(join([p.name for p in all_problems], ", "))")
                end
                MPI.Abort(comm, 1)
            end
        end
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        results = run_all_benchmarks(problems, comm, rank, size)
        
        if rank == 0
            # çµæœã®åˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            generate_benchmark_report(results)
            
            # çµæœã®ä¿å­˜
            timestamp = Dates.format(now(), "yyyymmdd_HHMMSS")
            filename = "benchmark_results_$(size)proc_$timestamp.json"
            save_benchmark_results(results, filename)
            
            println("\n=== Benchmark Complete ===")
            println("Total execution time: $(round(time(), digits=2)) seconds")
        end
        
    catch e
        if rank == 0
            println("Error during benchmark execution:")
            println(e)
        end
        MPI.Abort(comm, 1)
    end
    
    # MPIçµ‚äº†
    MPI.Finalize()
end

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã—ã¦å®Ÿè¡Œã•ã‚ŒãŸå ´åˆã®ã¿main()ã‚’å‘¼ã³å‡ºã—
if abspath(PROGRAM_FILE) == @__FILE__
    main()
end